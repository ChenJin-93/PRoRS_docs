{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分类过程\n",
    "\n",
    "## 最邻近分类：KNeighborsClassifier\n",
    "\n",
    "    from sklearn.neighbors import  KNeighborsClassifier\n",
    "    clf =KNeighborsClassifier()\n",
    "    clf.fit(X, Y)\n",
    "    P=meshgrid()\n",
    "    y=clf.predict(P)\n",
    "    cm_bright = ListedColormap(['Blue', 'Orange'])\n",
    "    plt.scatter(P[:,0], P[:,1], c=y, cmap=cm_bright)\n",
    "    \n",
    "    判别依据：dist=clf.kneighbors(P)[0]\n",
    "    kneighbors(X=None, n_neighbors=None, return_distance=True)\n",
    "    Returns indices of and distances to the neighbors of each point\n",
    "    \n",
    "<img src=\"Picture\\figure_3-1.png\" width = \"600\" height = \"400\" align=left>  <br>\n",
    "\n",
    "    图中第二列为不同数据集的最邻近分类结果。\n",
    "    \n",
    "    第三列为测试样本中相对于正类每个点到训练样本点的最邻近距离。\n",
    "    \n",
    "    第四列为测试样本中相对与负类每个点到训练样本点的最邻近距离。\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性判别分析：LDA\n",
    "\n",
    "    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "    clf =LinearDiscriminantAnalysis()\n",
    "    \n",
    "    判别依据：df=clf.decision_function(X) \n",
    "\n",
    "<img src=\"Picture\\figure_3-2.png\" width = \"450\" height = \"300\" align=left> <br>\n",
    "    \n",
    "    图中第二列为不同数据集的线性判别结果。\n",
    "    \n",
    "    第三列为测试样本中每个点到分类边界的距离。\n",
    "    距离为正则分类为正类；距离为负则分类为负类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 朴素贝叶斯：naive_bayes.GaussianNB\n",
    "\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    clf =GaussianNB()\n",
    "    clf.fit(X,Y)\n",
    "    y=clf.predict(P)\n",
    "    \n",
    "    判别依据：predict_proba(X)\n",
    "    Returns the probability of the samples for each class in the model. \n",
    "    The columns correspond to the classes in sorted order, as they appear in the attribute classes.\n",
    "    \n",
    "<img src=\"Picture\\figure_3-3.png\" width = \"600\" height = \"400\" align=left> <br>\n",
    "\n",
    "    图中第二列为不同数据集的高斯朴素贝叶斯的分类结果。\n",
    "    \n",
    "    第三列为测试样本中每个点相对于正类的概率。\n",
    "    \n",
    "    第四列为测试样本中每个点相对于负类的概率。\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 为什么naive_bayes.GaussianNB在不同数据集分类效果不同？\n",
    "    两类数据集看作两个服从高斯分布的数据集：\n",
    "    当两类数据集的高分分布协方差大致相同，高斯朴素贝叶斯分类边界呈现直线。\n",
    "    当两类数据集的高斯分布协方差不同，此时分类边界会偏向某一类数据集的均值中心。\n",
    "    \n",
    "<img src=\"Picture\\figure_3-4.png\" width = \"600\" height = \"400\" align=left>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "119px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
