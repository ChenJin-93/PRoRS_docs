{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1 组合分类概述\n",
    "\n",
    "## 5.1.1 AdaBoost\n",
    "\n",
    "![](Picture/figure_7-01.png)\n",
    "\n",
    "\n",
    "## 5.1.2 Bagging\n",
    "\n",
    "![](Picture/figure_7-02.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.3 决策树\n",
    "\n",
    "(1) 决策树分类器\n",
    "```python\n",
    "    from sklearn import tree\n",
    "    clf = tree.DecisionTreeClassifier(criterion='gini','entropy',max_depth=3)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) 信息增益\n",
    "\n",
    "![](Picture/figure_7-11.png)\n",
    "\n",
    "信息增益越大，用该属性进行划分所获得“纯度越大”。\n",
    "一般不直接使用信息增益，而是使用“增益率来选择最优划分属性”\n",
    "先从划分属性中找出信息增益高于平均水平的属性；\n",
    "再从中选择增益率最高的。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) 基尼系数\n",
    "\n",
    "![](Picture/figure_7-12.png)\n",
    "\n",
    "直观来说，Gini(D)反映了数据集D中随机抽取两个样本，器类别标记不一致的概率，因此基尼指数越小，则数据集纯度越高。\n",
    "选择使得划分后基尼指数最小的属性作为最优划分属性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.4 集成分类\n",
    "\n",
    "(1)  基分类器\n",
    "```python\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    clf=AdaBoostClassifier(base_estimator=DecisionTreeClassifier(criterion='gini',max_depth=3), n_estimators=5,learning_rate=1.0,algorithm='SAMME')\n",
    "\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    clf=RandomForestClassifier(n_estimators=10)\n",
    "```\n",
    "    将相互之间具有独立决策能力的分类器联合起来的方式就叫作集成分类器。  \n",
    "    事实证明通常情况下集成分类器的预测能力要比单个分类器的预测能力好得多。\n",
    "    \n",
    "![](Picture/figure_8-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) 随机森林分类过程\n",
    "\n",
    "    1.随机方式建立一个森林，森林里面有很多的决策树组成，每一棵决策树之间没有关联。\n",
    "    2.当有一个新的输入样本进入的时候，就让森林中的每一棵决策树分别进行分类。\n",
    "    3.判断样本应该属于哪一类，哪一类被选择最多，就预测这个样本为那一类。\n",
    "    \n",
    "    那随机森林具体如何构建？两个方面：数据的随机性选取，以及待选特征的随机选取。\n",
    "    设有N个样本，每个样本有M个features，决策树们其实都是随机地接受n个样本（对行随机取样）的m个feature（对列进行随机取样），每棵决策树的m个feature相同。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) Adaboost分类过程\n",
    "\n",
    "    其算法根据每次训练集之中每个样本的分类是否正确，以及上次的总体分类的准确率，来确定每个样本的权值。将修改过权值的新数据集送给下层进行训练，最后将每次训练得到的分类器最后融合起来，作为最后的决策分类器。\n",
    "    \n",
    "    (1) 训练样本的概率分布相当下，训练弱分类器\n",
    "    (2) 计算弱分类器的错误率；\n",
    "    (3) 选取合适阈值，使得误差最小；\n",
    "    (4) 更新样本权重；\n",
    "    \n",
    "```python\n",
    "    ABE =clf.estimators_\n",
    "    ABw =clf.estimator_weights_\n",
    "    Abr =clf.estimator_errors_\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "102px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
