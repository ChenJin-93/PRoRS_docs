{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 线性分类概述\n",
    "\n",
    "## 3.1.1 线性判别函数\n",
    "![](Picture/figure_3-01.png)\n",
    "\n",
    "## 3.1.2 两类问题\n",
    "![](Picture/figure_3-02.png)\n",
    "\n",
    "## 3.1.3 点到分类边界的距离\n",
    "![](Picture/figure_3-03.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 实验要求\n",
    "\n",
    "（1）根据不同损失函数的定义绘制训练样本的（归一化）损失值。\n",
    "\n",
    "（2）根据不同损失函数的定义绘制假设测试样本分别为正类或负类的（归一化）损失值。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 思考探索\n",
    "\n",
    "（1）不同损失函数的特点是什么？\n",
    "\n",
    "（2）损失函数在分类器参数学习中的作用是什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4 案例展示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4.1 损失函数\n",
    "\n",
    "损失函数与点到分类边界的距离有关：df=clf.decision_function(X) <br>\n",
    "hinge    :f=np.where(df < 1, 1 - df, 0) <br>        \n",
    "perceptron:f=-np.minimum(df, 0) <br>\n",
    "log     :f=np.log2(1 + np.exp(-df)) <br>\n",
    "squared_h :f=np.where(df< 1 ,1-df,0)^2  <br> \n",
    "modified_h:f=modified_huber_loss(df, 1) <br>\n",
    "\n",
    "![](Picture/figure_4-1.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4.2 不同损失函数下训练样本的损失值\n",
    "\n",
    "![](Picture/figure_4-2.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4.3 不同损失函数下测试样本（特征空间）的损失值\n",
    "\n",
    "![](Picture/figure_4-3.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4.4 各损失函数的特点\n",
    "\n",
    "Hinge loss ： margin 内有损失 边界的支持向量决定边界<br>\n",
    "Perceptron ： 分错有损失<br>\n",
    "Log loss_  ： 整体样本有损失  所有样本共同决定分类边界<br>\n",
    "    \n",
    "![](Picture/figure_4-4.png) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4.5 损失函数的作用\n",
    "\n",
    "损失函数估量模型的预测值与真实值的不一致程度---预测错误的程度。<br>\n",
    "损失函数越小，模型的鲁棒性就越好。<br>\n",
    "损失函数是经验风险函数的核心，也是结构风险函数重要组成部分，包括了经验风险项和正则项。<br>\n",
    "    \n",
    "![](Picture/figure_4-5.png) \n",
    "\n",
    "损失函数度量模型一次预测的好坏，风险函数（期望损失）度量平均意义下模型的好坏。<br>\n",
    "参数越多，模型越复杂，而越复杂的模型越容易过拟合。过拟合就是说模型在训练数据上的效果远远好于在测试集上的性能。此时可以考虑正则化，通过设置正则项前面的hyper parameter，来权衡损失函数和正则项，减小参数规模，达到模型简化的目的，从而使模型具有更好的泛化能力。\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "137px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
