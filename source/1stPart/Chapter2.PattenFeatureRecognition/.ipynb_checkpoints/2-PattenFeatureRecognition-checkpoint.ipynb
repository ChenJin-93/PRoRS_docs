{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-07T22:38:38.820584Z",
     "start_time": "2017-09-07T22:38:38.813574Z"
    }
   },
   "source": [
    "# 2.1模式特征分类概述"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2实验要求"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3思考探索"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4案例展示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4.1示例数据集\n",
    "示例数据集以Jupyter Notebook为基本工具。每个特征的取值范围为[0,255]，橙色表示+1类样本；蓝色表示-1类样本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) 数据集一：\n",
    "```python\n",
    "    from sklearn import datasets\n",
    "    X,Y=datasets.make_blobs(n_samples=n, centers=2, n_features=2, cluster_std=2, random_state=1)\n",
    "    cm_bright = ListedColormap(['Blue', 'Orange'])\n",
    "    plt.scatter(X[:,0], X[:,1], s=1, c=Y, cmap=cm_bright)\n",
    "```  \n",
    "![](Picture/figure_1-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-07T22:52:10.487734Z",
     "start_time": "2017-09-07T22:52:10.463717Z"
    }
   },
   "source": [
    "(2) 数据集二：\n",
    "```python\n",
    "    x= np.random.rand(n, 2); X=255*x\n",
    "    Y=[]\n",
    "    for i in range(0, n): \n",
    "        if (X[:,0][i]<=128 and X[:,1][i]<=128) or (X[:,0][i]>128 and X[:,1][i]>128):\n",
    "        Y.append(-1)\n",
    "        else:\n",
    "        Y.append(1)\n",
    "```       \n",
    "![](Picture/figure_1-2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3)  数据集三：\n",
    "```python\n",
    "    x,y= datasets.make_circles(n_samples=n, factor=0.05, noise=.2)\n",
    "```\n",
    "![](Picture/figure_1-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4)  数据集四：\n",
    "```python\n",
    "    t = 1 * np.pi * (0 + 4 * np.random.rand(1, n))\n",
    "    x0 = -t * np.cos(t); x2 = t * np.cos(t); x=np.concatenate((x0, x2), axis=1);\n",
    "    y0 = -t * np.sin(t); y2 = t * np.sin(t); y=np.concatenate((y0, y2), axis=1);\n",
    "    X = np.concatenate((x, y));\n",
    "    X += .3 * np.random.randn(2, 2*n);X = X.T;\n",
    "    X=255*MaxMinNormalization(X);\n",
    "    cs1=-1*np.ones(n); cs2=np.zeros(n); \n",
    "    Y=np.concatenate((cs1, cs2));\n",
    "```\n",
    "![](Picture/figure_1-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2.4.2模式分类\n",
    "    \n",
    "\n",
    "\n",
    "(1)  离散化特征空间，当测试点数量增加，分类结果是否有差异？\n",
    "\n",
    "![](Picture/figure_2-1.png)\n",
    "\n",
    "    利用所有训练样本以缺省参数学习分类器参数。\n",
    "    随机选取离散化特征空间中的每个点（256*256个点）进行分类。\n",
    "    图中第一列为不同数据集，第二到第五列分别代表从测试占整个特征空间的比例（0.01、0.05、0.1、1）。\n",
    "    当特征空间中用于测试的点数逐渐增加，可观察到在同种分类方法下（最邻近分类）的分类结果并无差异。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2)  同种分类器，对不同数据集进行两次分类的分类结果是否有差异？\n",
    "\n",
    "![](Picture/figure_2-2.png)\n",
    "\n",
    "    图中第一列为不同数据集，第二到第三列分别为第一次分类结果和第二次分类结果。\n",
    "    从图中结果可看出同种分类器下，两次分类的结果也并无差异。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3)  不同分类器，对不同数据集进行分类的分类结果会有什么样的差异？\n",
    "\n",
    "![](Picture\\figure_2-3.png)\n",
    "\n",
    "    图中第一列为不同数据集，第二到第五列分别为不同分类器（线性判别，高斯朴素贝叶斯，二次判别，最邻近方法）的分类结果.\n",
    "    从图中结果可看出不同分类器下，特征空间中的分类结果存在明显差异。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4) 为什么不同分类器，分类结果会有差异？\n",
    "    KNN：记住所有的训练样本，计算每个测试点到每个类别的所有训练点的距离，记录一个最近距离，对应的训练点类别，即是测试点的类别。\n",
    "\n",
    "    LDA：如果训练数据集的协方差矩阵一致，训练数据投影到一个维度上，得到决策边界，计算所有测试点到决策边界的距离，找出一个判断阈值，进而将所有测试点分类。\n",
    "\n",
    "    QDA：如果训练数据集的协方差矩阵不一致，得到的决策面是一个二次面。\n",
    "\n",
    "    GNB：根据训练集来估计类先验概率和每个特征的条件概率，然后利用训练出的模型，计算每个样本点的后验概率，求取最大后验概率，其对应的类别就是测试点的类别。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5)  分类器是什么？（映射函数？分类规则？分类边界？空间剖分？）\n",
    "    当分类器的类型、参数给定了之后，分类器就是对特征空间的剖分。\n",
    "    同一分类器，测试点数量增加、两次分类的结果都是相同的。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4.3分类过程\n",
    "\n",
    "(1)  最邻近分类：KNeighborsClassifier\n",
    "\n",
    "```python\n",
    "    from sklearn.neighbors import  KNeighborsClassifier\n",
    "    clf =KNeighborsClassifier()\n",
    "    clf.fit(X, Y)\n",
    "    P=meshgrid()\n",
    "    y=clf.predict(P)\n",
    "    cm_bright = ListedColormap(['Blue', 'Orange'])\n",
    "    plt.scatter(P[:,0], P[:,1], c=y, cmap=cm_bright)\n",
    "    \n",
    "    判别依据：dist=clf.kneighbors(P)[0]\n",
    "    kneighbors(X=None, n_neighbors=None, return_distance=True)\n",
    "    Returns indices of and distances to the neighbors of each point\n",
    "```   \n",
    "![](Picture/figure_3-1.png)\n",
    "\n",
    "    图中第二列为不同数据集的最邻近分类结果。\n",
    "    第三列为测试样本中相对于正类每个点到训练样本点的最邻近距离。\n",
    "    第四列为测试样本中相对与负类每个点到训练样本点的最邻近距离。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2)  线性判别分析：LDA\n",
    "```python\n",
    "    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "    clf =LinearDiscriminantAnalysis()\n",
    "   \n",
    "    判别依据：df=clf.decision_function(X) \n",
    "```\n",
    "![](Picture/figure_3-2.png)\n",
    "    \n",
    "    图中第二列为不同数据集的线性判别结果。\n",
    "    第三列为测试样本中每个点到分类边界的距离。\n",
    "    距离为正则分类为正类；距离为负则分类为负类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) 朴素贝叶斯：naive_bayes.GaussianNB\n",
    "```python\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    clf =GaussianNB()\n",
    "    clf.fit(X,Y)\n",
    "    y=clf.predict(P)\n",
    "    \n",
    "    判别依据：predict_proba(X)\n",
    "    Returns the probability of the samples for each class in the model. \n",
    "    The columns correspond to the classes in sorted order, as they appear in the attribute classes.\n",
    "```\n",
    "![](Picture/figure_3-3.png)\n",
    "\n",
    "    图中第二列为不同数据集的高斯朴素贝叶斯的分类结果。\n",
    "    第三列为测试样本中每个点相对于正类的概率。    \n",
    "    第四列为测试样本中每个点相对于负类的概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4) 为什么naive_bayes.GaussianNB在不同数据集分类效果不同？\n",
    "    两类数据集看作两个服从高斯分布的数据集：\n",
    "    当两类数据集的高分分布协方差大致相同，高斯朴素贝叶斯分类边界呈现直线。\n",
    "    当两类数据集的高斯分布协方差不同，此时分类边界会偏向某一类数据集的均值中心。\n",
    "    \n",
    "![](Picture/figure_3-4.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "30px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
